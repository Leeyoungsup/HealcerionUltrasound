{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import time\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import torchmetrics\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=24\n",
    "image_count=50\n",
    "img_size=224\n",
    "tf = ToTensor()\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    '../../data/segmentationDDH/train_aug_segmentation_dataset.csv')\n",
    "val_df = pd.read_csv(\n",
    "    '../../data/segmentationDDH/val_aug_segmentation_dataset.csv')\n",
    "\n",
    "train_img_list = train_df['file name'].to_list()\n",
    "train_label_list = train_df['standard mask'].to_list()\n",
    "train_case_list = train_df['case'].to_list()\n",
    "train_img_path = '../../data/segmentationDDH/aug_dataset/train/'\n",
    "val_img_list = val_df['file name'].to_list()\n",
    "val_label_list = val_df['standard mask'].to_list()\n",
    "val_case_list = val_df['case'].to_list()\n",
    "val_img_path = '../../data/segmentationDDH/aug_dataset/val/'\n",
    "\n",
    "val_image=torch.zeros((len(val_img_list),3,img_size,img_size))\n",
    "val_mask=torch.zeros((len(val_img_list),3,img_size,img_size),dtype=torch.uint8)    \n",
    "train_image=torch.zeros((len(train_img_list),3,img_size,img_size))\n",
    "train_mask=torch.zeros((len(train_img_list),3,img_size,img_size),dtype=torch.uint8)\n",
    "\n",
    "for i in tqdm(range(len(train_img_list))):\n",
    "    train_image[i] = tf(np.array(Image.open(\n",
    "        train_img_path+str(train_case_list[i])+'/image/'+train_img_list[i]).resize((img_size, img_size))))\n",
    "    train_mask[i,1]=tf(np.array(Image.open(\n",
    "        train_img_path+str(train_case_list[i])+'/mask/'+str(train_label_list[i]).zfill(5)+'/1'+train_img_list[i][train_img_list[i].find('_'):]).resize((img_size, img_size))))\n",
    "    train_mask[i,2]=tf(np.array(Image.open(\n",
    "        train_img_path+str(train_case_list[i])+'/mask/'+str(train_label_list[i]).zfill(5)+'/2'+train_img_list[i][train_img_list[i].find('_'):]).resize((img_size, img_size))))\n",
    "    train_mask[i,0]=torch.where((train_mask[i,1]+train_mask[i,2])==0,1,0)\n",
    "    \n",
    "\n",
    "for i in tqdm(range(len(val_img_list))):\n",
    "    val_image[i] = tf(np.array(Image.open(\n",
    "        val_img_path+str(val_case_list[i])+'/image/'+val_img_list[i]).resize((img_size, img_size))))\n",
    "    val_mask[i,1]=tf(np.array(Image.open(\n",
    "        val_img_path+str(val_case_list[i])+'/mask/'+str(val_label_list[i]).zfill(5)+'/1.png').resize((img_size, img_size))))\n",
    "    val_mask[i,2]=tf(np.array(Image.open(\n",
    "        val_img_path+str(val_case_list[i])+'/mask/'+str(val_label_list[i]).zfill(5)+'/2.png').resize((img_size, img_size))))\n",
    "    val_mask[i,0]=torch.where((val_mask[i,1]+val_mask[i,2])==0,1,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        label_path =  self.label[idx]\n",
    "        return image_path, label_path\n",
    "    \n",
    "train_dataset = CustomDataset(train_image, train_mask[:,1:])\n",
    "\n",
    "val_dataset = CustomDataset(val_image, val_mask[:,1:])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=smp.UnetPlusPlus('mobilenet_v2', in_channels=3, classes=2).to(device)\n",
    "summary(model,(batch_size,3,img_size,img_size))\n",
    "def dice_loss(pred, target, num_classes=2):\n",
    "    smooth = 1.\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] = 1 - (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return torch.mean(dice_per_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "val_acc_list=[]\n",
    "MIN_loss=5000\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)\n",
    "metrics = defaultdict(float)\n",
    "for epoch in range(300):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    for x, y in train:\n",
    "        model.train()\n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).to(device)\n",
    "        cost = dice_loss(predict, y,num_classes=2) # cost 구함\n",
    "        acc=1-dice_loss(predict, y,num_classes=2)\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.step() \n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        y = y.to('cpu')\n",
    "\n",
    "        x=x.to('cpu')\n",
    "        train.set_description(f\"epoch: {epoch+1}/{300} Step: {count+1} dice_loss : {running_loss/count:.4f} dice_score: {1-running_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "#validation\n",
    "    val=tqdm(validation_dataloader)\n",
    "    model.eval()\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            \n",
    "            predict = model(x).to(device)\n",
    "            cost = dice_loss(predict, y,num_classes=2) # cost 구함\n",
    "            acc=1-dice_loss(predict, y,num_classes=2)\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            y = y.to('cpu')\n",
    "            x=x.to('cpu')\n",
    "            val.set_description(f\"Validation epoch: {epoch+1}/{300} Step: {count+1} dice_loss : {val_running_loss/count:.4f}  dice_score: {1-val_running_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), '../../model/segmentation/NestedUNet_callback.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "        \n",
    "    if epoch%20==5:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1) \n",
    "        plt.title('loss_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "        plt.plot(np.arange(epoch+1),val_loss_list,label='validation_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)  \n",
    "        plt.title('acc_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_acc_list,label='train_acc')\n",
    "        plt.plot(np.arange(epoch+1),val_acc_list,label='validation_acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.title('loss_graph')\n",
    "plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "plt.plot(np.arange(epoch+1),val_loss_list,label='validation_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0, 1]) \n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)  \n",
    "plt.title('acc_graph')\n",
    "plt.plot(np.arange(epoch+1),train_acc_list,label='train_acc')\n",
    "plt.plot(np.arange(epoch+1),val_acc_list,label='validation_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim([0, 1]) \n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('batch size= 4')\n",
    "print('image size= 224')\n",
    "print('learning rate= 0.0001')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
