{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import roifile\n",
    "import cv2\n",
    "\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=glob(\"../../data/ddh_standard_frame_data/*\")\n",
    "\n",
    "def createDirectory(directory):\n",
    "    \"\"\"_summary_\n",
    "        create Directory\n",
    "    Args:\n",
    "        directory (string): file_path\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "        \n",
    "def expand2square(pil_img, background_color):\n",
    "    \"\"\"_summary_\n",
    "        pillow image make Image to be padded\n",
    "    Args:\n",
    "        pil_img (Pillow image): Image to be padded\n",
    "        background_color (_type_): \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .zip zipper and .Roi file open & save\n",
    "for i in range(1,21):\n",
    "    file_list=glob('../../data/pre_DDH/'+str(i)+'/seg_label/*.zip')\n",
    "    output_dir = \"../../data/pre_DDH/\"+str(i)+\"/segLabel/\"\n",
    "    format = \"zip\"\n",
    "    for j in range(len(file_list)):\n",
    "        createDirectory(output_dir+os.path.splitext(os.path.basename(file_list[j]))[0][:os.path.splitext(os.path.basename(file_list[j]))[0].find('_')])\n",
    "        shutil.unpack_archive(file_list[j], output_dir+os.path.splitext(os.path.basename(file_list[j]))[0][:os.path.splitext(os.path.basename(file_list[j]))[0].find('_')], format)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygon to mask\n",
    "roi_list=glob('../../data/pre_DDH/**/segLabel/**')\n",
    "image_list=[f.replace('/segLabel', '/Standard')+'.bmp' for f in roi_list]\n",
    "mask_list=[f.replace('/segLabel', '/raw_seg_mask') for f in roi_list]\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    createDirectory(mask_list[i])\n",
    "    for j in range(1,4):\n",
    "        try:\n",
    "            roi = roifile.ImagejRoi.fromfile(roi_list[i]+'/'+str(j)+'.roi')\n",
    "        except:\n",
    "            try:\n",
    "                roi = roifile.ImagejRoi.fromfile(roi_list[i]+'/'+str(j)+'-1.roi')\n",
    "            except:\n",
    "                break;\n",
    "        \n",
    "        image=Image.open(image_list[i])\n",
    "        polygon=roi.integer_coordinates\n",
    "        polygon[:,0]+=roi.left\n",
    "        polygon[:,1]+=roi.top\n",
    "        mask= np.zeros((image.height,image.width))\n",
    "        mask=cv2.fillPoly(mask,[polygon],255)\n",
    "        cv2.imwrite(mask_list[i]+'/'+str(j)+'.png',mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2b3000dcda47b18b1a39b84ada2474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22759736923451795214eb500273d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#classification dataset create\n",
    "standard_image_list=glob('../../data/pre_DDH/**/Standard/*.bmp')\n",
    "not_standard_image_list=glob('../../data/pre_DDH/**/Not_Standard/*.bmp')\n",
    "padding_standard_image_list=[f.replace('/pre_DDH', '/classificationDDH/dataset') for f in standard_image_list]\n",
    "padding_standard_image_list=[f.replace('.bmp', '.png') for f in padding_standard_image_list]\n",
    "padding_standard_image_list=[f.replace('/Standard', '') for f in padding_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('/pre_DDH', '/classificationDDH/dataset') for f in not_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('.bmp', '.png') for f in padding_not_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('/Not_Standard', '') for f in padding_not_standard_image_list]\n",
    "image_size=512\n",
    "case_list=[]\n",
    "file_list=[]\n",
    "class_list=[]\n",
    "for i in tqdm(range(len(standard_image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(standard_image_list[i])))\n",
    "    file_name=os.path.basename(padding_standard_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_standard_image_list[i]))\n",
    "    image=expand2square(Image.open(standard_image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_standard_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(1)\n",
    "    \n",
    "for i in tqdm(range(len(not_standard_image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(not_standard_image_list[i])))\n",
    "    file_name=os.path.basename(padding_not_standard_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_not_standard_image_list[i]))\n",
    "    image=expand2square(Image.open(not_standard_image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_not_standard_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(0)\n",
    "classification_dataset=pd.DataFrame({'case':case_list,'file name':file_list,'standard class':class_list})\n",
    "classification_dataset.to_csv('../../data/classificationDDH/classification_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification augmentation dataset create\n",
    "standard_image_list=glob('../../data/pre_DDH/**/Standard/*.bmp')\n",
    "not_standard_image_list=glob('../../data/pre_DDH/**/Not_Standard/*.bmp')\n",
    "padding_standard_image_list=[f.replace('/pre_DDH', '/classificationDDH/aug_dataset') for f in standard_image_list]\n",
    "padding_standard_image_list=[f.replace('.bmp', '.png') for f in padding_standard_image_list]\n",
    "padding_standard_image_list=[f.replace('/Standard', '') for f in padding_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('/pre_DDH', '/classificationDDH/aug_dataset') for f in not_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('.bmp', '.png') for f in padding_not_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('/Not_Standard', '') for f in padding_not_standard_image_list]\n",
    "image_size=512\n",
    "case_list=[]\n",
    "file_list=[]\n",
    "class_list=[]\n",
    "for i in tqdm(range(len(standard_image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(standard_image_list[i])))\n",
    "    file_name=os.path.basename(padding_standard_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_standard_image_list[i]))\n",
    "    image=expand2square(Image.open(standard_image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_standard_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(1)\n",
    "    \n",
    "for i in tqdm(range(len(not_standard_image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(not_standard_image_list[i])))\n",
    "    file_name=os.path.basename(padding_not_standard_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_not_standard_image_list[i]))\n",
    "    image=expand2square(Image.open(not_standard_image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_not_standard_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(0)\n",
    "aug_classification_dataset=pd.DataFrame({'case':case_list,'file name':file_list,'standard class':class_list})\n",
    "aug_classification_dataset.to_csv('../../data/classificationDDH/aug_classification_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_not_standard_image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fc286f74894827a6a0ad9482cacabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#segmentation dataset create\n",
    "image_list=glob('../../data/pre_DDH/**/Standard/*.bmp')\n",
    "mask_list=[f.replace('.bmp', '') for f in image_list]\n",
    "mask_list=[f.replace('/Standard', '/raw_seg_mask') for f in mask_list]\n",
    "padding_image_list=[f.replace('/pre_DDH', '/segmentationDDH/dataset') for f in image_list]\n",
    "padding_image_list=[f.replace('.bmp', '.png') for f in padding_image_list]\n",
    "padding_image_list=[f.replace('/Standard', '/image') for f in padding_image_list]\n",
    "\n",
    "padding_mask_list=[f.replace('/pre_DDH', '/segmentationDDH/dataset') for f in mask_list]\n",
    "padding_mask_list=[f.replace('/raw_seg_mask', '/mask') for f in padding_mask_list]\n",
    "\n",
    "image_size=512\n",
    "case_list=[]\n",
    "file_list=[]\n",
    "class_list=[]\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(image_list[i])))\n",
    "    file_name=os.path.basename(padding_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_image_list[i]))\n",
    "    createDirectory(padding_mask_list[i])\n",
    "    image=expand2square(Image.open(image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(os.path.splitext(file_name)[0])\n",
    "    for j in range(1,4):\n",
    "        \n",
    "        mask=expand2square(Image.open(mask_list[i]+'/'+str(j)+'.png'),(0))\n",
    "        mask=mask.resize((image_size,image_size))\n",
    "        mask.save(padding_mask_list[i]+'/'+str(j)+'.png')\n",
    "segmentation_dataset=pd.DataFrame({'case':case_list,'file name':file_list,'standard mask':class_list})\n",
    "segmentation_dataset.to_csv('../../data/segmentationDDH/segmentation_dataset.csv',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/segmenationDDH/dataset/2/mask'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(padding_mask_list[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
