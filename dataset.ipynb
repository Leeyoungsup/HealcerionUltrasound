{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import roifile\n",
    "import cv2\n",
    "import random\n",
    "import imgaug.augmenters as iaa\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=glob(\"../../data/ddh_standard_frame_data/*\")\n",
    "\n",
    "def createDirectory(directory):\n",
    "    \"\"\"_summary_\n",
    "        create Directory\n",
    "    Args:\n",
    "        directory (string): file_path\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "        \n",
    "def expand2square(pil_img, background_color):\n",
    "    \"\"\"_summary_\n",
    "        pillow image make Image to be padded\n",
    "    Args:\n",
    "        pil_img (Pillow image): Image to be padded\n",
    "        background_color (_type_): \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=glob('../../data/relabel/**/seg_label/*.zip')\n",
    "image_list=glob('../../data/relabel/**/Standard/*.bmp')\n",
    "\n",
    "label_list=[f.replace('/seg_label', '/Standard') for f in label_list]\n",
    "label_list=[f.replace('.zip', '.bmp') for f in label_list]\n",
    "image_list_copy=list(set(image_list)-set(label_list))\n",
    "image_list_copy1=[f.replace('/Standard', '/Not_Standard') for f in image_list_copy]\n",
    "for i in range(len(image_list_copy)):\n",
    "    shutil.move(image_list_copy[i],image_list_copy1[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .zip zipper and .Roi file open & save\n",
    "for i in range(1,21):\n",
    "    file_list=glob('../../data/relabel/'+str(i)+'/seg_label/*.zip')\n",
    "    output_dir = \"../../data/relabel/\"+str(i)+\"/segLabel/\"\n",
    "    format = \"zip\"\n",
    "    for j in range(len(file_list)):\n",
    "        createDirectory(output_dir+os.path.splitext(os.path.basename(file_list[j]))[0])\n",
    "        shutil.unpack_archive(file_list[j], output_dir+os.path.splitext(os.path.basename(file_list[j]))[0], format)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=glob('../../data/relabel/**/segLabel/**/2.roi')\n",
    "label_list1=[f.replace('/relabel', '/pre_DDH') for f in label_list]\n",
    "label_list2=[f.replace('/2.roi', '/1.roi') for f in label_list]\n",
    "for i in range(len(label_list)):\n",
    "    try:\n",
    "        shutil.copy(label_list1[i],label_list2[i]) \n",
    "    except:\n",
    "        shutil.copy(label_list1[i].replace('/2.roi','/2-1.roi'),label_list2[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cabb5500e041efa6282f1f0d6dcb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# polygon to mask\n",
    "roi_list=glob('../../data/relabel/**/segLabel/**')\n",
    "image_list=[f.replace('/segLabel', '/Standard')+'.bmp' for f in roi_list]\n",
    "mask_list=[f.replace('/segLabel', '/raw_seg_mask') for f in roi_list]\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    createDirectory(mask_list[i])\n",
    "    for j in range(1,5):\n",
    "        roi = roifile.ImagejRoi.fromfile(roi_list[i]+'/'+str(j)+'.roi')\n",
    "        image=Image.open(image_list[i])\n",
    "        polygon=roi.integer_coordinates\n",
    "        polygon[:,0]+=roi.left\n",
    "        polygon[:,1]+=roi.top\n",
    "        mask= np.zeros((image.height,image.width))\n",
    "        mask=cv2.fillPoly(mask,[polygon],255)\n",
    "        cv2.imwrite(mask_list[i]+'/'+str(j)+'.png',mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12bd1aab36545169d263891b6e5558c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022de2176bf8472d9259851d880d8c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#classification dataset create\n",
    "standard_image_list=glob('../../data/relabel/**/Standard/*.bmp')\n",
    "not_standard_image_list=glob('../../data/relabel/**/Not_Standard/*.bmp')\n",
    "padding_standard_image_list=[f.replace('/relabel', '/classificationDDH/dataset') for f in standard_image_list]\n",
    "padding_standard_image_list=[f.replace('.bmp', '.png') for f in padding_standard_image_list]\n",
    "padding_standard_image_list=[f.replace('/Standard', '') for f in padding_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('/relabel', '/classificationDDH/dataset') for f in not_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('.bmp', '.png') for f in padding_not_standard_image_list]\n",
    "padding_not_standard_image_list=[f.replace('/Not_Standard', '') for f in padding_not_standard_image_list]\n",
    "image_size=512\n",
    "case_list=[]\n",
    "file_list=[]\n",
    "class_list=[]\n",
    "for i in tqdm(range(len(standard_image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(standard_image_list[i])))\n",
    "    file_name=os.path.basename(padding_standard_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_standard_image_list[i]))\n",
    "    image=expand2square(Image.open(standard_image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_standard_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(1)\n",
    "    \n",
    "for i in tqdm(range(len(not_standard_image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(not_standard_image_list[i])))\n",
    "    file_name=os.path.basename(padding_not_standard_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_not_standard_image_list[i]))\n",
    "    image=expand2square(Image.open(not_standard_image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_not_standard_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(0)\n",
    "classification_dataset=pd.DataFrame({'case':case_list,'file name':file_list,'standard class':class_list})\n",
    "classification_dataset.to_csv('../../data/classificationDDH/classification_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a5cfeb83140cca58cdb98026cf6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#classification augmentation dataset create\n",
    "df_dataset=pd.read_csv('../../data/classificationDDH/classification_dataset.csv')\n",
    "file_path='../../data/classificationDDH/dataset/'\n",
    "aug_file_path='../../data/classificationDDH/aug_dataset/'\n",
    "train_case_list=[]\n",
    "train_file_list=[]\n",
    "train_class_list=[]\n",
    "test_case_list=[]\n",
    "test_file_list=[]\n",
    "test_class_list=[]\n",
    "val_case_list=[]\n",
    "val_file_list=[]\n",
    "val_class_list=[]\n",
    "def classification_image_aug(image):\n",
    "    src_image=np.array(image)\n",
    "    images=[src_image]*6\n",
    "    images[1]=iaa.fliplr(images[1])\n",
    "    images[2]=iaa.flipud(images[2])\n",
    "    images[3]=iaa.fliplr(images[3])\n",
    "    images[3]=iaa.flipud(images[3])\n",
    "    r=random.randrange(1,360)\n",
    "    images[4]=iaa.Rotate((r))(image=images[4]) \n",
    "    r=random.randrange(1,360)\n",
    "    images[5]=iaa.Rotate((r))(image=images[5]) \n",
    "    return images  \n",
    "for i in tqdm(range(len(df_dataset))):\n",
    "    case=df_dataset.loc[i]['case']\n",
    "    file_name=df_dataset.loc[i]['file name']\n",
    "    label=df_dataset.loc[i]['standard class']\n",
    "    file_raw_name=os.path.splitext(file_name)[0]\n",
    "    if case==19 or case==20:\n",
    "        set_name='test'\n",
    "        image=Image.open(file_path+str(case)+'/'+file_name)\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case))\n",
    "        test_case_list.append(case)\n",
    "        test_class_list.append(label)\n",
    "        test_file_list.append(file_raw_name+'.png')\n",
    "        image.save(aug_file_path+set_name+'/'+str(case)+'/'+file_raw_name+'.png')\n",
    "    elif case==17 or case==18:\n",
    "        set_name='val'\n",
    "        image=Image.open(file_path+str(case)+'/'+file_name)\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case))\n",
    "        val_case_list.append(case)\n",
    "        val_class_list.append(label)\n",
    "        val_file_list.append(file_raw_name+'.png')\n",
    "        image.save(aug_file_path+set_name+'/'+str(case)+'/'+file_raw_name+'.png')\n",
    "    else:\n",
    "        set_name='train'\n",
    "        image=Image.open(file_path+str(case)+'/'+file_name)\n",
    "        augmented_images = classification_image_aug(image)\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case))\n",
    "        for k in range(6):\n",
    "            train_case_list.append(case)\n",
    "            train_class_list.append(label)\n",
    "            train_file_list.append(file_raw_name+'_'+str(k)+'.png')\n",
    "            Image.fromarray(augmented_images[k]).save(aug_file_path+set_name+'/'+str(case)+'/'+file_raw_name+'_'+str(k)+'.png')\n",
    "        \n",
    "train_aug_classification_dataset=pd.DataFrame({'case':train_case_list,'file name':train_file_list,'standard class':train_class_list})\n",
    "train_aug_classification_dataset.to_csv('../../data/classificationDDH/train_aug_classification_dataset.csv',index=False)\n",
    "test_aug_classification_dataset=pd.DataFrame({'case':test_case_list,'file name':test_file_list,'standard class':test_class_list})\n",
    "test_aug_classification_dataset.to_csv('../../data/classificationDDH/test_aug_classification_dataset.csv',index=False)\n",
    "val_aug_classification_dataset=pd.DataFrame({'case':val_case_list,'file name':val_file_list,'standard class':val_class_list})\n",
    "val_aug_classification_dataset.to_csv('../../data/classificationDDH/val_aug_classification_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e379e5e728f46299eb1114ef384b231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#segmentation dataset create\n",
    "image_list=glob('../../data/relabel/**/Standard/*.bmp')\n",
    "mask_list=[f.replace('.bmp', '') for f in image_list]\n",
    "mask_list=[f.replace('/Standard', '/raw_seg_mask') for f in mask_list]\n",
    "padding_image_list=[f.replace('/relabel', '/segmentationDDH/dataset') for f in image_list]\n",
    "padding_image_list=[f.replace('.bmp', '.png') for f in padding_image_list]\n",
    "padding_image_list=[f.replace('/Standard', '/image') for f in padding_image_list]\n",
    "\n",
    "padding_mask_list=[f.replace('/relabel', '/segmentationDDH/dataset') for f in mask_list]\n",
    "padding_mask_list=[f.replace('/raw_seg_mask', '/mask') for f in padding_mask_list]\n",
    "\n",
    "image_size=512\n",
    "case_list=[]\n",
    "file_list=[]\n",
    "class_list=[]\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    case=os.path.basename(os.path.dirname(os.path.dirname(image_list[i])))\n",
    "    file_name=os.path.basename(padding_image_list[i])\n",
    "    createDirectory(os.path.dirname(padding_image_list[i]))\n",
    "    createDirectory(padding_mask_list[i])\n",
    "    image=expand2square(Image.open(image_list[i]),(0,0,0))\n",
    "    image=image.resize((image_size,image_size))\n",
    "    image.save(padding_image_list[i])\n",
    "    case_list.append(case)\n",
    "    file_list.append(file_name)\n",
    "    class_list.append(os.path.splitext(file_name)[0])\n",
    "    for j in range(1,5):\n",
    "        \n",
    "        mask=expand2square(Image.open(mask_list[i]+'/'+str(j)+'.png'),(0))\n",
    "        mask=mask.resize((image_size,image_size))\n",
    "        mask.save(padding_mask_list[i]+'/'+str(j)+'.png')\n",
    "segmentation_dataset=pd.DataFrame({'case':case_list,'file name':file_list,'standard mask':class_list})\n",
    "segmentation_dataset.to_csv('../../data/segmentationDDH/segmentation_dataset.csv',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ed84b566de41288aecd5bf2b2031ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#segmentation augmentation dataset create\n",
    "df_dataset=pd.read_csv('../../data/segmentationDDH/segmentation_dataset.csv')\n",
    "file_path='../../data/segmentationDDH/dataset/'\n",
    "aug_file_path='../../data/segmentationDDH/aug_dataset/'\n",
    "train_case_list=[]\n",
    "train_file_list=[]\n",
    "train_class_list=[]\n",
    "test_case_list=[]\n",
    "test_file_list=[]\n",
    "test_class_list=[]\n",
    "val_case_list=[]\n",
    "val_file_list=[]\n",
    "val_class_list=[]\n",
    "def segmentation_image_aug(image,mask1,mask2,mask3,mask4):\n",
    "    src_image=np.array(image)\n",
    "    src_mask1=np.array(mask1)\n",
    "    src_mask2=np.array(mask2)\n",
    "    src_mask3=np.array(mask3)\n",
    "    src_mask4=np.array(mask4)\n",
    "    images=[src_image]*6\n",
    "    masks=[src_mask1]*6\n",
    "    masks1=[src_mask2]*6\n",
    "    masks2=[src_mask3]*6\n",
    "    masks3=[src_mask4]*6\n",
    "    masks[1]=iaa.fliplr(masks[1])\n",
    "    masks[2]=iaa.flipud(masks[2])\n",
    "    masks[3]=iaa.fliplr(masks[3])\n",
    "    masks[3]=iaa.flipud(masks[3])\n",
    "    images[1]=iaa.fliplr(images[1])\n",
    "    images[2]=iaa.flipud(images[2])\n",
    "    images[3]=iaa.fliplr(images[3])\n",
    "    images[3]=iaa.flipud(images[3])\n",
    "    masks1[1]=iaa.fliplr(masks1[1])\n",
    "    masks1[2]=iaa.flipud(masks1[2])\n",
    "    masks1[3]=iaa.fliplr(masks1[3])\n",
    "    masks1[3]=iaa.flipud(masks1[3])\n",
    "    masks2[1]=iaa.fliplr(masks2[1])\n",
    "    masks2[2]=iaa.flipud(masks2[2])\n",
    "    masks2[3]=iaa.fliplr(masks2[3])\n",
    "    masks2[3]=iaa.flipud(masks2[3])\n",
    "    masks3[1]=iaa.fliplr(masks3[1])\n",
    "    masks3[2]=iaa.flipud(masks3[2])\n",
    "    masks3[3]=iaa.fliplr(masks3[3])\n",
    "    masks3[3]=iaa.flipud(masks3[3])\n",
    "    r=random.randrange(1,360)\n",
    "    images[4]=iaa.Rotate((r))(image=images[4]) \n",
    "    masks[4]=iaa.Rotate((r))(image=masks[4])\n",
    "    masks1[4]=iaa.Rotate((r))(image=masks1[4]) \n",
    "    masks2[4]=iaa.Rotate((r))(image=masks2[4])\n",
    "    masks3[4]=iaa.Rotate((r))(image=masks3[4]) \n",
    "    r=random.randrange(1,360)\n",
    "    images[5]=iaa.Rotate((r))(image=images[5]) \n",
    "    masks[5]=iaa.Rotate((r))(image=masks[5])\n",
    "    masks1[5]=iaa.Rotate((r))(image=masks1[5])\n",
    "    masks2[5]=iaa.Rotate((r))(image=masks2[5])\n",
    "    masks3[5]=iaa.Rotate((r))(image=masks3[5])\n",
    "    return images,masks,masks1 ,masks2,masks3 \n",
    " \n",
    "for i in tqdm(range(len(df_dataset))):\n",
    "    case=df_dataset.loc[i]['case']\n",
    "    file_name=df_dataset.loc[i]['file name']\n",
    "    label=df_dataset.loc[i]['standard mask']\n",
    "    file_raw_name=os.path.splitext(file_name)[0]\n",
    "    if case==19 or case==20:\n",
    "        set_name='test'\n",
    "        image=Image.open(file_path+str(case)+'/image/'+file_name)\n",
    "        mask1=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/1.png')\n",
    "        mask2=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/2.png')\n",
    "        mask3=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/3.png')\n",
    "        mask4=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/4.png')\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case)+'/image/')\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/')\n",
    "        test_case_list.append(case)\n",
    "        test_class_list.append(str(file_raw_name))\n",
    "        test_file_list.append(file_raw_name+'.png')\n",
    "        image.save(aug_file_path+set_name+'/'+str(case)+'/image/'+str(file_raw_name)+'.png')\n",
    "        mask1.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'1.png')\n",
    "        mask2.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'2.png')\n",
    "        mask3.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'3.png')\n",
    "        mask4.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'4.png')\n",
    "    elif case==17 or case==18:\n",
    "        set_name='val'\n",
    "        image=Image.open(file_path+str(case)+'/image/'+file_name)\n",
    "        mask1=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/1.png')\n",
    "        mask2=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/2.png')\n",
    "        mask3=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/3.png')\n",
    "        mask4=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/4.png')\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case)+'/image/')\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/')\n",
    "        \n",
    "        val_case_list.append(case)\n",
    "        val_class_list.append(str(file_raw_name))\n",
    "        val_file_list.append(file_raw_name+'.png')\n",
    "        image.save(aug_file_path+set_name+'/'+str(case)+'/image/'+str(file_raw_name)+'.png')\n",
    "        mask1.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'1.png')\n",
    "        mask2.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'2.png')\n",
    "        mask3.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'3.png')\n",
    "        mask4.save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'4.png')\n",
    "    else:\n",
    "        set_name='train'\n",
    "        image=Image.open(file_path+str(case)+'/image/'+file_name)\n",
    "        mask1=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/1.png')\n",
    "        mask2=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/2.png')\n",
    "        mask3=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/3.png')\n",
    "        mask4=Image.open(file_path+str(case)+'/mask/'+str(file_raw_name)+'/4.png')\n",
    "        augmented_images,augmented_masks,augmented_masks1,augmented_masks2,augmented_masks3 = segmentation_image_aug(image,mask1,mask2,mask3,mask4)\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case)+'/image/')\n",
    "        createDirectory(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/')\n",
    "        for k in range(6):\n",
    "            train_case_list.append(case)\n",
    "            train_class_list.append(os.path.splitext(file_name)[0])\n",
    "            train_file_list.append(file_raw_name+'_'+str(k)+'.png')\n",
    "            Image.fromarray(augmented_images[k]).save(aug_file_path+set_name+'/'+str(case)+'/image/'+str(file_raw_name)+'_'+str(k)+'.png')\n",
    "            Image.fromarray(augmented_masks[k]).save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'1_'+str(k)+'.png')\n",
    "            Image.fromarray(augmented_masks1[k]).save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'2_'+str(k)+'.png')\n",
    "            Image.fromarray(augmented_masks2[k]).save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'3_'+str(k)+'.png')\n",
    "            Image.fromarray(augmented_masks3[k]).save(aug_file_path+set_name+'/'+str(case)+'/mask/'+str(file_raw_name)+'/'+'4_'+str(k)+'.png')\n",
    "        \n",
    "train_aug_segmentation_dataset=pd.DataFrame({'case':train_case_list,'file name':train_file_list,'standard mask':train_class_list})\n",
    "train_aug_segmentation_dataset.to_csv('../../data/segmentationDDH/train_aug_segmentation_dataset.csv',index=False)\n",
    "test_aug_segmentation_dataset=pd.DataFrame({'case':test_case_list,'file name':test_file_list,'standard mask':test_class_list})\n",
    "test_aug_segmentation_dataset.to_csv('../../data/segmentationDDH/test_aug_segmentation_dataset.csv',index=False)\n",
    "val_aug_segmentation_dataset=pd.DataFrame({'case':val_case_list,'file name':val_file_list,'standard mask':val_class_list})\n",
    "val_aug_segmentation_dataset.to_csv('../../data/segmentationDDH/val_aug_segmentation_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "case=df_dataset.loc[i]['case']\n",
    "file_name=df_dataset.loc[i]['file name']\n",
    "label=df_dataset.loc[i]['standard mask']\n",
    "mask=np.array(Image.open(file_path+str(case)+'/'+'mask/'+str(label).zfill(5)+'/3.png'))\n",
    "image=np.array(Image.open(file_path+str(case)+'/'+'image/'+file_name).convert('L'))\n",
    "image[np.where(mask!=255)]=0\n",
    "\n",
    "plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image = np.array(Image.open(file_path+str(case)+'/'+file_name).convert('L'))\n",
    "\n",
    "\n",
    "f_transform = np.fft.fft2(image)\n",
    "f_transform_shifted = np.fft.fftshift(f_transform)\n",
    "\n",
    "\n",
    "magnitude_spectrum = 20 * np.log(np.abs(f_transform_shifted))\n",
    "\n",
    "\n",
    "plt.subplot(121), plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(122), plt.imshow(magnitude_spectrum, cmap='gray')\n",
    "plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
