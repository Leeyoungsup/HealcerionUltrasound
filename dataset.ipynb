{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=glob(\"../../data/ddh_standard_frame_data/*\")\n",
    "\n",
    "\n",
    "def AnnotationsPd(annotations):\n",
    "    values_list = []\n",
    "    names_list = [] \n",
    "    df = pd.DataFrame(annotations['tags'])\n",
    "    for row in df['attributes']:\n",
    "        values_list.append(row[0]['value'])\n",
    "        names_list.append(row[0]['name'])\n",
    "\n",
    "    df=df.drop(['attributes'],axis=1)   \n",
    "    df.insert(3,'value',values_list)\n",
    "    df.insert(4,'name',names_list)\n",
    "    return df\n",
    "\n",
    "def DatasetCreate(folder_path):\n",
    "    data_img_path=glob(folder_path+\"/data/*.jpg\")\n",
    "    data_img_path.sort()\n",
    "    standard_file_path=[]\n",
    "    with open(folder_path+'/annotations.json') as f:\n",
    "        annotations_json = json.load(f)\n",
    "    std_frame=AnnotationsPd(annotations_json[0])['frame']\n",
    "    for frame in std_frame:\n",
    "        file= data_img_path[frame]\n",
    "        standard_file_path.append(file)\n",
    "    not_standard_file_path=list(set(data_img_path)-set(standard_file_path))   \n",
    "    standard_file_path=list(set(standard_file_path))\n",
    "    return standard_file_path,not_standard_file_path\n",
    "\n",
    "def create_folder(temp_path):\n",
    "    try:\n",
    "        os.mkdir(temp_path)\n",
    "    except FileExistsError as e:\n",
    "        pass  # 에러를 무시한다.\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n",
    "def image_preprocessing(image_path):\n",
    "    image=Image.open(image_path)\n",
    "    np_image=np.array(image)\n",
    "    if np_image[520,520].mean()==0:\n",
    "        image=image.crop((196,128,508,640))\n",
    "        image=image.resize((312,512))\n",
    "        image=expand2square(image,(0,0,0))\n",
    "    else:\n",
    "        image=image.crop((176,128,536,640))\n",
    "        image=image.resize((360,512))\n",
    "        image=expand2square(image,(0,0,0))\n",
    "    image=image.resize((256,256))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_file_path=[]\n",
    "Train_file_path=[]\n",
    "Val_file_path=[]\n",
    "Test_class=[]\n",
    "Train_class=[]\n",
    "Val_class=[]\n",
    "for i in range(20):\n",
    "    if i<=15:\n",
    "        if i==7 or i==8:\n",
    "            continue \n",
    "        set_name='train'\n",
    "        temp_path='../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)\n",
    "        create_folder(temp_path)\n",
    "        standard_file_path,not_standard_file_path=DatasetCreate(folder_path[i])\n",
    "        for j in range(len(standard_file_path)):\n",
    "            image=image_preprocessing(standard_file_path[j])\n",
    "            Train_file_path.append('/p'+str(i+1)+'/standard_'+str(j).zfill(5)+'.jpg')\n",
    "            Train_class.append(1)\n",
    "            image.save('../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)+'/standard_'+str(j).zfill(5)+'.jpg')\n",
    "            \n",
    "        for j in range(len(not_standard_file_path)):\n",
    "            image=image_preprocessing(not_standard_file_path[j])\n",
    "            Train_file_path.append('/p'+str(i+1)+'/not_standard_'+str(j).zfill(5)+'.jpg')\n",
    "            Train_class.append(0)\n",
    "            image.save('../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)+'/not_standard_'+str(j).zfill(5)+'.jpg')\n",
    "    elif i<=17:\n",
    "        set_name='val'\n",
    "        temp_path='../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)\n",
    "        create_folder(temp_path)\n",
    "        standard_file_path,not_standard_file_path=DatasetCreate(folder_path[i])\n",
    "        for j in range(len(standard_file_path)):\n",
    "            image=image_preprocessing(standard_file_path[j])\n",
    "            Val_file_path.append('/p'+str(i+1)+'/standard_'+str(j).zfill(5)+'.jpg')\n",
    "            Val_class.append(1)\n",
    "            image.save('../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)+'/standard_'+str(j).zfill(5)+'.jpg')\n",
    "            \n",
    "        for j in range(len(not_standard_file_path)):\n",
    "            image=image_preprocessing(not_standard_file_path[j])\n",
    "            Val_file_path.append('/p'+str(i+1)+'/not_standard_'+str(j).zfill(5)+'.jpg')\n",
    "            Val_class.append(0)\n",
    "            image.save('../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)+'/not_standard_'+str(j).zfill(5)+'.jpg')\n",
    "    else:\n",
    "        set_name='test'\n",
    "        temp_path='../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)\n",
    "        create_folder(temp_path)\n",
    "        standard_file_path,not_standard_file_path=DatasetCreate(folder_path[i])\n",
    "        for j in range(len(standard_file_path)):\n",
    "            image=image_preprocessing(standard_file_path[j])\n",
    "            Test_file_path.append('/p'+str(i+1)+'/standard_'+str(j).zfill(5)+'.jpg')\n",
    "            Test_class.append(1)\n",
    "            image.save('../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)+'/standard_'+str(j).zfill(5)+'.jpg')\n",
    "            \n",
    "        for j in range(len(not_standard_file_path)):\n",
    "            image=image_preprocessing(not_standard_file_path[j])\n",
    "            Test_file_path.append('/p'+str(i+1)+'/not_standard_'+str(j).zfill(5)+'.jpg')\n",
    "            Test_class.append(0)\n",
    "            image.save('../../data/standardFrame_data/scale_skip/'+set_name+'/p'+str(i+1)+'/not_standard_'+str(j).zfill(5)+'.jpg')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_dataframe = {'file_path' : Test_file_path, 'standard' : Test_class}\n",
    "Train_dataframe = {'file_path' : Train_file_path, 'standard' : Train_class}\n",
    "Validation_dataframe = {'file_path' : Val_file_path, 'standard' : Val_class}\n",
    "Test_dataframe = pd.DataFrame(Test_dataframe)\n",
    "Train_dataframe = pd.DataFrame(Train_dataframe)\n",
    "Validation_dataframe = pd.DataFrame(Validation_dataframe)\n",
    "Test_dataframe.to_csv('../../data/standardFrame_data/scale_skip/Test_dataframe.csv',mode = 'w', index=False)\n",
    "Train_dataframe.to_csv('../../data/standardFrame_data/scale_skip/Train_dataframe.csv',mode = 'w', index=False)\n",
    "Validation_dataframe.to_csv('../../data/standardFrame_data/scale_skip/Validation_dataframe.csv',mode = 'w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
